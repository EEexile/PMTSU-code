#ç§»é™¤é—¨æ§æœºåˆ¶ 
#ç¬¬5ä¸ª
import os
import sys
from pathlib import Path

import hydra 
from omegaconf import DictConfig

import torch
from torch import nn, optim

from data import load_ssl_features, train_valid_test_iemocap_dataloader

from utils import compute_unweighted_accuracy, compute_weighted_f1

import logging
import numpy as np
import torch.nn.functional as F
logger = logging.getLogger('IEMOCAP_Downstream')

import torch
from torch import nn
from torch.nn import MultiheadAttention


class BaseModel(nn.Module):
    def __init__(self, 
                 input_dim: int = 768, 
                 hidden_dim: int = 256, 
                 num_classes: int = 4, 
                 num_heads: int = 4):
        super().__init__()
        
        # 1. å…±äº«ç‰¹å¾æå–å±‚
        self.shared_layer = nn.Sequential(
            nn.Linear(input_dim, hidden_dim*2),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # 2. ä»»åŠ¡é€‚é…å±‚
        self.emo_adapter = nn.Linear(hidden_dim*2, hidden_dim)
        self.vad_adapters = nn.ModuleDict({
            'valence': nn.Linear(hidden_dim*2, hidden_dim),
            'arousal': nn.Linear(hidden_dim*2, hidden_dim),
            'dominance': nn.Linear(hidden_dim*2, hidden_dim)
        })
        
        # 3. è¾…åŠ©ä»»åŠ¡è¾“å‡ºå±‚
        self.valence_head = nn.Linear(hidden_dim, 3)
        self.arousal_head = nn.Linear(hidden_dim, 3)
        self.dominance_head = nn.Linear(hidden_dim, 3)
        
        # 4. äº¤å‰æ³¨æ„åŠ›æ¨¡å—
        self.cross_attention = CrossTaskAttention(hidden_dim, num_heads)
        
        
        # 6. ä¸»ä»»åŠ¡è¾“å‡ºå±‚
        self.emotion_head = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x: torch.Tensor, padding_mask: torch.Tensor = None):
        # å…±äº«ç‰¹å¾æå–
        shared_features = self.shared_layer(x)  # [B, L, 512]
        
        # ä¸»ä»»åŠ¡ç‰¹å¾
        emo_feat = self._get_emo_feature(shared_features, padding_mask)  # [B, 256]
        
        # è¾…åŠ©ä»»åŠ¡å¤„ç†
        vad_feats, valence_out, arousal_out, dominance_out = self._get_vad_features(
            shared_features, padding_mask)  # [B,3,256]
        
        # äº¤å‰æ³¨æ„åŠ›
        attn_out, attn_weights = self.cross_attention(emo_feat, vad_feats)  # [B,3,256]
        attn_out = attn_out.squeeze(1)
        # é—¨æ§èåˆ
        fused_feature = (emo_feat + attn_out)/2.0
        
        fusion_weights = None
        
        # æ®‹å·®è¿æ¥ä¿è¯æ¢¯åº¦æµåŠ¨
        fused_feature = fused_feature + emo_feat
        
        # ä¸»ä»»åŠ¡é¢„æµ‹
        emotion_out = self.emotion_head(fused_feature)
        
        return emotion_out, valence_out, arousal_out, dominance_out, attn_weights, fusion_weights
    
    def _get_emo_feature(self, features, mask):

        emo_feat = self.emo_adapter(features) 
        emo_feat = nn.functional.relu(emo_feat)
        """å¤„ç†ä¸»ä»»åŠ¡ç‰¹å¾"""
        if mask is not None:
            mask_float = mask.unsqueeze(-1).float()
            valid = emo_feat * (1 - mask_float)
            emo_feat = valid.sum(dim=1) / (1 - mask_float).sum(dim=1)
        else:
            emo_feat = emo_feat.mean(dim=1)
        return emo_feat 
    
    def _get_vad_features(self, features, mask):
        """å¤„ç†ä¸‰ä¸ªVADä»»åŠ¡"""
        vad_feats = []
        outputs = []
        
        for task, adapter in self.vad_adapters.items():
            # ä»»åŠ¡é€‚é…
            task_feat = adapter(features)  # [B, L, H]
            task_feat = nn.functional.relu(task_feat)
            
            # å¤„ç†æ©ç 
            if mask is not None:
                mask_float = mask.unsqueeze(-1).float()
                valid_feat = task_feat * (1 - mask_float)
                avg_feat = valid_feat.sum(dim=1) / (1 - mask_float).sum(dim=1)
            else:
                avg_feat = task_feat.mean(dim=1)
            
            vad_feats.append(avg_feat)
            
            # ä»»åŠ¡ç‰¹å®šè¾“å‡º
            if task == 'valence':
                outputs.append(self.valence_head(avg_feat))
            elif task == 'arousal':
                outputs.append(self.arousal_head(avg_feat))
            else:
                outputs.append(self.dominance_head(avg_feat))
        
        return torch.stack(vad_feats, dim=1), *outputs

# ------------------ æ¨¡å—å®ç° ------------------
class CrossTaskAttention(nn.Module):
    def __init__(self, hidden_dim, num_heads):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.emo_query = nn.Linear(hidden_dim, hidden_dim)
        self.vad_proj = nn.Linear(hidden_dim, hidden_dim*2)  # Key+Value
        self.attn = MultiheadAttention(
            hidden_dim, num_heads, batch_first=True)
        
    def forward(self, emo_feat, vad_feats):
        Q = self.emo_query(emo_feat).unsqueeze(1)  # [B,1,H]
        KV = self.vad_proj(vad_feats)  # [B,3,2H]
        K, V = torch.split(KV, self.hidden_dim, dim=-1)  # ä½¿ç”¨ self.hidden_dim
        
        attn_out, attn_weights = self.attn(Q, K, V, need_weights=True)
        return attn_out, attn_weights 


def count_parameters(model):
    total_params = 0
    for name, parameter in model.named_parameters():
        param = parameter.numel()
        print(f"{name}: {param}")
        total_params += param
    print(f"\nTotal number of parameters: {total_params}")

def calculate_class_weights(labels, num_classes, device='cpu'):
    """è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡ï¼ˆåŸºäºç±»åˆ«é¢‘ç‡çš„å€’æ•°ï¼‰"""
    labels = torch.tensor(labels, device=device)
    class_counts = torch.bincount(labels, minlength=num_classes).float()
    class_weights = 1.0 / (class_counts + 1e-6)  # åŠ å°å€¼é˜²æ­¢é™¤é›¶
    class_weights = class_weights / class_weights.sum()  # å½’ä¸€åŒ–
    return class_weights

def get_vad_weights(train_loader, num_classes, device='cpu'):
    """è®¡ç®—VADä¸‰ä¸ªç»´åº¦çš„ç±»åˆ«æƒé‡"""
    # æ”¶é›†æ‰€æœ‰VADæ ‡ç­¾
    all_valence = []
    all_arousal = []
    all_dominance = []
    
    for batch in train_loader:
        vad_labels = batch["vad_labels"]
        all_valence.extend(vad_labels[:, 0].tolist())
        all_arousal.extend(vad_labels[:, 1].tolist())
        all_dominance.extend(vad_labels[:, 2].tolist())
    
    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æƒé‡
    valence_weights = calculate_class_weights(all_valence, num_classes, device)
    arousal_weights = calculate_class_weights(all_arousal, num_classes, device)
    dominance_weights = calculate_class_weights(all_dominance, num_classes, device)
    
    return [valence_weights, arousal_weights, dominance_weights]

@torch.no_grad()
def validate_and_test(model, data_loader, device, num_classes):
    model.eval()
    correct, total = 0, 0

    # æƒ…æ„Ÿä»»åŠ¡æŒ‡æ ‡
    emotion_correct, emotion_total = 0, 0
    emotion_unweighted_correct = [0] * num_classes
    emotion_unweighted_total = [0] * num_classes
    
    # === æ–°å¢ï¼šç”¨äºè®¡ç®— F1 çš„ç»Ÿè®¡é‡ ===
    tp = [0] * num_classes  # true positives
    fp = [0] * num_classes  # false positives
    fn = [0] * num_classes  # false negatives

    # VADä»»åŠ¡æŒ‡æ ‡
    vad_correct = [0, 0, 0]  # valence, arousal, dominance
    vad_total = [0, 0, 0]

    # æ³¨æ„åŠ›å’Œé—¨æ§æƒé‡ï¼ˆå¯é€‰ï¼Œä¸å½±å“F1ï¼‰
    all_attn_weights = []


    for batch in data_loader:
        ids, net_input, labels = batch["id"], batch["net_input"], batch["labels"]
        feats = net_input["feats"]
        speech_padding_mask = net_input["padding_mask"]
        vad_targets = batch["vad_labels"].to(device)

        feats = feats.to(device)
        speech_padding_mask = speech_padding_mask.to(device)
        labels = labels.to(device)

        emotion_out, valence_out, arousal_out, dominance_out, attn_weights, fusion_weights = model(feats, speech_padding_mask)
         
        # æƒ…æ„Ÿé¢„æµ‹
        _, emotion_pred = torch.max(emotion_out.data, 1)
        emotion_total += labels.size(0)
        emotion_correct += (emotion_pred == labels).sum().item()
        
        # VADé¢„æµ‹
        _, valence_pred = torch.max(valence_out.data, 1)
        _, arousal_pred = torch.max(arousal_out.data, 1)
        _, dominance_pred = torch.max(dominance_out.data, 1)
        
        vad_total[0] += labels.size(0)
        vad_total[1] += labels.size(0)
        vad_total[2] += labels.size(0)
        vad_correct[0] += (valence_pred == vad_targets[:, 0]).sum().item()
        vad_correct[1] += (arousal_pred == vad_targets[:, 1]).sum().item()
        vad_correct[2] += (dominance_pred == vad_targets[:, 2]).sum().item()
        
        # æ›´æ–° UA å’Œ F1 æ‰€éœ€çš„ç»Ÿè®¡é‡
        for i in range(len(labels)):
            true_label = labels[i].item()
            pred_label = emotion_pred[i].item()
            
            emotion_unweighted_total[true_label] += 1
            if pred_label == true_label:
                emotion_unweighted_correct[true_label] += 1
                tp[true_label] += 1
            else:
                fp[pred_label] += 1
                fn[true_label] += 1

        # æ”¶é›†æƒé‡ï¼ˆå¯é€‰ï¼‰
        all_attn_weights.append(attn_weights.squeeze(1).cpu().numpy())


    # === è®¡ç®—æŒ‡æ ‡ ===
    emotion_wa = emotion_correct / emotion_total * 100
    emotion_ua = compute_unweighted_accuracy(emotion_unweighted_correct, emotion_unweighted_total) * 100
    
    # âœ… è®¡ç®— weighted F1
    weighted_f1 = compute_weighted_f1(tp, fp, fn, emotion_unweighted_total) * 100

    vad_acc = [vad_correct[i] / vad_total[i] * 100 for i in range(3)]

    # ï¼ˆå¯é€‰ï¼‰æ‰“å°æ³¨æ„åŠ›åˆ†æ
    if all_attn_weights:
        all_attn_weights = np.concatenate(all_attn_weights, axis=0)
        avg_attn_weights = all_attn_weights.mean(axis=0)
        print(f"\nğŸ“Š Cross-Attention Weights: Valence={avg_attn_weights[0]:.4f}, "
              f"Arousal={avg_attn_weights[1]:.4f}, Dominance={avg_attn_weights[2]:.4f}")

    return emotion_wa, emotion_ua, weighted_f1, vad_acc

def train_one_epoch(model, optimizer, criterion, train_loader, device, task_weights, vad_weights=None):
    model.train()
    total_loss = 0
    emotion_loss_total = 0
    # åˆ†åˆ«è®°å½•ä¸‰ä¸ªVADä»»åŠ¡çš„æŸå¤±
    valence_loss_total = 0
    arousal_loss_total = 0
    dominance_loss_total = 0
    
    for batch in train_loader:
        ids, net_input, labels = batch["id"], batch["net_input"], batch["labels"]
        feats = net_input["feats"]
        speech_padding_mask = net_input["padding_mask"]

        feats = feats.to(device)
        speech_padding_mask = speech_padding_mask.to(device)
        labels = labels.to(device)
        vad_labels = batch["vad_labels"].to(device)  # [B, 3]
        
        optimizer.zero_grad()
        
        # æ¨¡å‹å‰å‘ä¼ æ’­
        emotion_out, valence_out, arousal_out, dominance_out,_, _ = model(feats, speech_padding_mask)
        
        # è®¡ç®—æƒ…æ„Ÿä»»åŠ¡æŸå¤±
        emotion_loss = criterion(emotion_out, labels.long())
        
        # è®¡ç®—VADä»»åŠ¡æŸå¤±ï¼ˆå¸¦æƒé‡ï¼‰
        if vad_weights is not None:
            # åŠ æƒäº¤å‰ç†µæŸå¤±
            valence_loss = F.cross_entropy(
                valence_out, vad_labels[:, 0].long(),
                weight=vad_weights[0]
            )
            arousal_loss = F.cross_entropy(
                arousal_out, vad_labels[:, 1].long(),
                weight=vad_weights[1]
            )
            dominance_loss = F.cross_entropy(
                dominance_out, vad_labels[:, 2].long(),
                weight=vad_weights[2]
            )
        else:
            # æ™®é€šäº¤å‰ç†µæŸå¤±
            valence_loss = criterion(valence_out, vad_labels[:, 0].long())
            arousal_loss = criterion(arousal_out, vad_labels[:, 1].long())
            dominance_loss = criterion(dominance_out, vad_labels[:, 2].long())
        
        # åŠ æƒæ€»æŸå¤±
        total_loss_batch = (
            task_weights[0] * emotion_loss +
            task_weights[1] * valence_loss +
            task_weights[2] * arousal_loss +
            task_weights[3] * dominance_loss
        )
        
        total_loss_batch.backward()
        optimizer.step()
        
        # ç´¯åŠ å„é¡¹æŸå¤±
        total_loss += total_loss_batch.item()
        emotion_loss_total += emotion_loss.item()
        valence_loss_total += valence_loss.item()
        arousal_loss_total += arousal_loss.item()
        dominance_loss_total += dominance_loss.item()
    
    # è®¡ç®—å¹³å‡æŸå¤±
    num_batches = len(train_loader)
    return (
        total_loss / num_batches, 
        emotion_loss_total / num_batches,
        [  # è¿”å›VADä¸‰ä¸ªç»´åº¦çš„å•ç‹¬æŸå¤±
            valence_loss_total / num_batches,
            arousal_loss_total / num_batches,
            dominance_loss_total / num_batches
        ]
    )

@hydra.main(config_path='config', config_name='default.yaml')
def train_iemocap(cfg: DictConfig):
    # æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„
    emotion_dict = {'ang': 0, 'hap': 1, 'neu': 2, 'sad': 3}
    # VADæ ‡ç­¾æ˜ å°„ (å‡è®¾VADå·²ç»æ˜¯0-3çš„ç¦»æ•£æ ‡ç­¾)
    vad_dict = {'0': 0, '1': 1, '2': 2}  
    
    n_samples = [1085, 1023, 1151, 1031, 1241]  # Session1, 2, 3, 4, 5
    idx_sessions = [0, 1, 2, 3, 4]

    test_wa_avg, test_ua_avg, test_vad_acc_avg = 0., 0., [0., 0., 0.]
    test_f1_avg = 0.0

    for fold in idx_sessions:  # extract the $fold$th as test set
        # torch.manual_seed(cfg.common.seed)    
        #éšæœºç§å­è®¾ç«‹
        seed = cfg.common.seed
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        
        logger.info(f"------Now it's {fold+1}th fold------")
        
        val_wa_history = [] 
        prev_emo_loss = 1.0  # åˆå§‹ä¸»ä»»åŠ¡æŸå¤±
        prev_vad_losses = [1.0, 1.0, 1.0]  # åˆå§‹VADæŸå¤±  
        
        device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")
        torch.cuda.empty_cache()
        
        # åŠ è½½æ•°æ®é›†
        dataset = load_ssl_features(cfg.dataset.feat_path, emotion_dict)

        test_len = n_samples[fold] 
        test_idx_start = sum(n_samples[:fold])
        test_idx_end = test_idx_start + test_len 
        train_loader, val_loader, test_loader = train_valid_test_iemocap_dataloader(
            dataset,
            cfg.dataset.batch_size,
            test_idx_start,
            test_idx_end,
            eval_is_test=cfg.dataset.eval_is_test,
        )

        model = BaseModel(input_dim=768, num_classes=len(emotion_dict))
        model = model.to(device)

        # è®¡ç®—VADç±»åˆ«æƒé‡
        vad_num_classes = len(vad_dict)
        vad_weights = get_vad_weights(train_loader, vad_num_classes, device)
        logger.info(f"VAD Class Weights - Valence: {vad_weights[0].tolist()}")
        logger.info(f"VAD Class Weights - Arousal: {vad_weights[1].tolist()}")
        logger.info(f"VAD Class Weights - Dominance: {vad_weights[2].tolist()}")

        
        optimizer = optim.Adam(
            model.parameters(), 
            lr=cfg.optimization.lr, 
            weight_decay=cfg.optimization.get('weight_decay', 1e-4)  # æ·»åŠ æƒé‡è¡°å‡
        )
        scheduler = optim.lr_scheduler.CyclicLR(
            optimizer, 
            base_lr=cfg.optimization.lr, 
            max_lr=3e-3, 
            step_size_up=20,
            cycle_momentum=False 
        )
        criterion = nn.CrossEntropyLoss()

        # å®šä¹‰è®­ç»ƒé˜¶æ®µå‚æ•°
        phase1_epochs = 5  # é˜¶æ®µ1çš„epochæ•°
        phase2_total_epochs = max(0, cfg.optimization.epoch - phase1_epochs)
        
        # è®¡ç®—é˜¶æ®µ2çš„è§£å†»æ­¥éª¤
        if phase2_total_epochs > 0:
            step_interval = phase2_total_epochs // 2
            step1_epoch = phase1_epochs  # å¼€å§‹è§£å†»vad_adapters
            step2_epoch = phase1_epochs + step_interval  # å¼€å§‹è§£å†»cross_attention
            


        best_val_wa = 0
        best_val_wa_epoch = 0
        save_dir = os.path.join(str(Path.cwd()), f"model_{fold+1}.pth")
        
        for epoch in range(cfg.optimization.epoch):
            
            # åŠ¨æ€è°ƒæ•´å‚æ•°å†»ç»“çŠ¶æ€å’Œä»»åŠ¡æƒé‡
            if epoch < phase1_epochs:
                # é˜¶æ®µ1: ä»…è®­ç»ƒä¸»ä»»åŠ¡ï¼Œå†»ç»“æ‰€æœ‰VADç›¸å…³å‚æ•°
                # å†»ç»“VADé€‚é…å™¨
                for param in model.vad_adapters.parameters():
                    param.requires_grad = False
                # å†»ç»“VADè¾“å‡ºå±‚
                for param in model.valence_head.parameters():
                    param.requires_grad = False
                for param in model.arousal_head.parameters():
                    param.requires_grad = False
                for param in model.dominance_head.parameters():
                    param.requires_grad = False
                # å†»ç»“äº¤å‰æ³¨æ„åŠ›
                for param in model.cross_attention.parameters():
                    param.requires_grad = False
                
                # ä»…ä½¿ç”¨æƒ…æ„Ÿä»»åŠ¡æŸå¤±
                task_weights = [1.0, 0.0, 0.0, 0.0]
                logger.info(f"Epoch {epoch+1}: PHASE 1 - Only training emotion task, freezing all VAD components")
            else:   
               # é˜¶æ®µ2: æ¸è¿›å¼è§£å†»å‚æ•°
                if epoch < step2_epoch:
                    # æ­¥éª¤1: è§£å†»vad_adaptersï¼ˆä»»åŠ¡é€‚é…å±‚ï¼‰
                    for param in model.vad_adapters.parameters():
                        param.requires_grad = True
                    for param in model.valence_head.parameters():
                        param.requires_grad = True
                    for param in model.arousal_head.parameters():
                        param.requires_grad = True
                    for param in model.dominance_head.parameters():
                        param.requires_grad = True
                    # ä¿æŒcross_attentionå’Œfusionå†»ç»“
                    for param in model.cross_attention.parameters():
                        param.requires_grad = False

                    logger.info(f"Epoch {epoch+1}: PHASE 2 - Step 1: Unfrozen VAD adapters")
                else:
                    # æ­¥éª¤2: è§£å†»cross_attentionï¼ˆæ³¨æ„åŠ›æ¨¡å—ï¼‰
                    for param in model.cross_attention.parameters():
                        param.requires_grad = True

                    logger.info(f"Epoch {epoch+1}: PHASE 2 - Step 2: Unfrozen cross-attention")


                
     
                phase2_epoch = epoch - phase1_epochs
                if phase2_total_epochs > 0:
                    ratio = min(phase2_epoch / phase2_total_epochs, 1.0)
                else:
                    ratio = 1.0
                
                w_emotion = 1 - 0.4 * ratio  
                total_vad_weight = 0.0 + 0.4 * ratio  
                w_v = w_a = w_d = total_vad_weight / 3.0  
                
                task_weights = [w_emotion, w_v, w_a, w_d]
                logger.info(f"Epoch {epoch+1}: PHASE 2 - Task Weights: Emo={w_emotion:.2f}, VAD Total={total_vad_weight:.2f} (V={w_v:.2f}, A={w_a:.2f}, D={w_d:.2f})")
            
            # è®­ç»ƒæ—¶ä¼ å…¥VADæƒé‡
            total_loss, emotion_loss, vad_loss = train_one_epoch(
                model, optimizer, criterion, train_loader, device, 
                task_weights, vad_weights
            )
            scheduler.step()
            v_loss, a_loss, d_loss = vad_loss 
            
            # æ›´æ–°æŸå¤±è®°å½•
            prev_emo_loss = emotion_loss
            prev_vad_losses = [v_loss, a_loss, d_loss] 

            # éªŒè¯
            val_wa, val_ua, val_f1, val_vad_acc = validate_and_test(
                model, val_loader, device, num_classes=len(emotion_dict)
            )
            val_wa_history.append(val_wa)

            # æ—¥å¿—è¾“å‡ºæƒé‡ä¿¡æ¯
            logger.info(f"Epoch {epoch+1} Task Weights: "
                        f"Emo={task_weights[0]:.2f}, "
                        f"Val={task_weights[1]:.2f}, "
                        f"Aro={task_weights[2]:.2f}, "
                        f"Dom={task_weights[3]:.2f}")

            
            if val_wa > best_val_wa:
                best_val_wa = val_wa
                best_val_wa_epoch = epoch
                torch.save(model.state_dict(), save_dir)

            # ä¿®æ”¹åçš„æ—¥å¿—è¯­å¥
            logger.info(f"Epoch {epoch+1}: "
                        f"Total Loss: {total_loss:.4f}, "
                        f"Emotion Loss: {emotion_loss:.4f}, "
                        f"VAD Loss: V={vad_loss[0]:.4f}, A={vad_loss[1]:.4f}, D={vad_loss[2]:.4f}, "  # åˆ†åˆ«è®¿é—®åˆ—è¡¨å…ƒç´ 
                        f"Val WA: {val_wa:.3f}%, "
                        f"Val UA: {val_ua:.3f}%, "
                        f"Val F1: {val_f1:.3f}%, "  # âœ… æ–°å¢ F1
                        f"Val VAD Acc: V={val_vad_acc[0]:.3f}%, A={val_vad_acc[1]:.3f}%, D={val_vad_acc[2]:.3f}%")

        # æµ‹è¯•
        ckpt = torch.load(save_dir)
        model.load_state_dict(ckpt, strict=True)
        test_wa, test_ua,test_f1, test_vad_acc = validate_and_test(
            model, test_loader, device, num_classes=len(emotion_dict)
        )
        
        logger.info(f"The {fold+1}th Fold at epoch {best_val_wa_epoch + 1}: "
                    f"Test WA {test_wa:.3f}%, "
                    f"Test UA {test_ua:.3f}%, "
                    f"Test F1 {test_f1:.3f}%, "  # âœ… æ–°å¢ F1
                    f"Test VAD Acc: V={test_vad_acc[0]:.3f}%, A={test_vad_acc[1]:.3f}%, D={test_vad_acc[2]:.3f}%")
        
        test_wa_avg += test_wa
        test_ua_avg += test_ua
        test_f1_avg += test_f1
        test_vad_acc_avg = [test_vad_acc_avg[i] + test_vad_acc[i] for i in range(3)]

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    num_folds = len(idx_sessions)
    test_wa_avg /= num_folds
    test_ua_avg /= num_folds
    test_vad_acc_avg = [acc / num_folds for acc in test_vad_acc_avg]

    logger.info(f"Average Results: "
                f"WA: {test_wa_avg:.3f}%, "
                f"UA: {test_ua_avg:.3f}%, "
                f"F1: {test_f1_avg / len(idx_sessions):.3f}%, " 
                f"VAD Acc: V={test_vad_acc_avg[0]:.3f}%, A={test_vad_acc_avg[1]:.3f}%, D={test_vad_acc_avg[2]:.3f}%")

if __name__ == '__main__':
    train_iemocap()

