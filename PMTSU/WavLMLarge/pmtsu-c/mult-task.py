#åŸºå‡†ä»£ç  è¾…åŠ©ä»»åŠ¡ä¸ºå›å½’ä»»åŠ¡

import os
import sys
from pathlib import Path

import hydra 
from omegaconf import DictConfig

import torch
from torch import nn, optim

from data import load_ssl_features, train_valid_test_iemocap_dataloader

from utils import compute_unweighted_accuracy, compute_weighted_f1

import logging
import numpy as np
import torch.nn.functional as F
logger = logging.getLogger('IEMOCAP_Downstream')

import torch
from torch import nn
from torch.nn import MultiheadAttention


class BaseModel(nn.Module):
    def __init__(self, 
                 input_dim: int = 768, 
                 hidden_dim: int = 256, 
                 num_classes: int = 4, 
                 num_heads: int = 4):
        super().__init__()
        
        # 1. å…±äº«ç‰¹å¾æå–å±‚
        self.shared_layer = nn.Sequential(
            nn.Linear(input_dim, hidden_dim*2),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # 2. ä»»åŠ¡é€‚é…å±‚
        self.emo_adapter = nn.Linear(hidden_dim*2, hidden_dim)
        self.vad_adapters = nn.ModuleDict({
            'valence': nn.Linear(hidden_dim*2, hidden_dim),
            'arousal': nn.Linear(hidden_dim*2, hidden_dim),
            'dominance': nn.Linear(hidden_dim*2, hidden_dim)
        })
        
        # 3. è¾…åŠ©ä»»åŠ¡è¾“å‡ºå±‚
        self.valence_head = nn.Linear(hidden_dim, 1)
        self.arousal_head = nn.Linear(hidden_dim, 1)
        self.dominance_head = nn.Linear(hidden_dim, 1)
        
        # 4. äº¤å‰æ³¨æ„åŠ›æ¨¡å—
        self.cross_attention = CrossTaskAttention(hidden_dim, num_heads)
        
        # 5. é—¨æ§èåˆæ¨¡å—
        self.fusion = GatedFusion(hidden_dim)
        
        # 6. ä¸»ä»»åŠ¡è¾“å‡ºå±‚
        self.emotion_head = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x: torch.Tensor, padding_mask: torch.Tensor = None):
        # å…±äº«ç‰¹å¾æå–
        shared_features = self.shared_layer(x)  # [B, L, 512]
        
        # ä¸»ä»»åŠ¡ç‰¹å¾
        emo_feat = self._get_emo_feature(shared_features, padding_mask)  # [B, 256]
        
        # è¾…åŠ©ä»»åŠ¡å¤„ç†
        vad_feats, valence_out, arousal_out, dominance_out = self._get_vad_features(
            shared_features, padding_mask)  # [B,3,256]
        
        # äº¤å‰æ³¨æ„åŠ›
        attn_out, attn_weights = self.cross_attention(emo_feat, vad_feats)  # [B,3,256]
        
        # é—¨æ§èåˆ
        fused_feature, fusion_weights = self.fusion(attn_out, emo_feat)  # [B,256]
        
        # æ®‹å·®è¿æ¥ä¿è¯æ¢¯åº¦æµåŠ¨
        fused_feature = fused_feature + emo_feat
        
        # ä¸»ä»»åŠ¡é¢„æµ‹
        emotion_out = self.emotion_head(fused_feature)
        
        return emotion_out, valence_out, arousal_out, dominance_out, attn_weights, fusion_weights
    
    def _get_emo_feature(self, features, mask):

        emo_feat = self.emo_adapter(features) 
        emo_feat = nn.functional.relu(emo_feat)
        """å¤„ç†ä¸»ä»»åŠ¡ç‰¹å¾"""
        if mask is not None:
            mask_float = mask.unsqueeze(-1).float()
            valid = emo_feat * (1 - mask_float)
            emo_feat = valid.sum(dim=1) / (1 - mask_float).sum(dim=1)
        else:
            emo_feat = emo_feat.mean(dim=1)
        return emo_feat 
    
    def _get_vad_features(self, features, mask):
        """å¤„ç†ä¸‰ä¸ªVADä»»åŠ¡"""
        vad_feats = []
        outputs = []
        
        for task, adapter in self.vad_adapters.items():
            # ä»»åŠ¡é€‚é…
            task_feat = adapter(features)  # [B, L, H]
            task_feat = nn.functional.relu(task_feat)
            
            # å¤„ç†æ©ç 
            if mask is not None:
                mask_float = mask.unsqueeze(-1).float()
                valid_feat = task_feat * (1 - mask_float)
                avg_feat = valid_feat.sum(dim=1) / (1 - mask_float).sum(dim=1)
            else:
                avg_feat = task_feat.mean(dim=1)
            
            vad_feats.append(avg_feat)
            
            # ä»»åŠ¡ç‰¹å®šè¾“å‡º
            if task == 'valence':
                outputs.append(self.valence_head(avg_feat))
            elif task == 'arousal':
                outputs.append(self.arousal_head(avg_feat))
            else:
                outputs.append(self.dominance_head(avg_feat))
        
        return torch.stack(vad_feats, dim=1), *outputs

# ------------------ æ¨¡å—å®ç° ------------------
class CrossTaskAttention(nn.Module):
    def __init__(self, hidden_dim, num_heads):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.emo_query = nn.Linear(hidden_dim, hidden_dim)
        self.vad_proj = nn.Linear(hidden_dim, hidden_dim*2)  # Key+Value
        self.attn = MultiheadAttention(
            hidden_dim, num_heads, batch_first=True)
        
    def forward(self, emo_feat, vad_feats):
        Q = self.emo_query(emo_feat).unsqueeze(1)  # [B,1,H]
        KV = self.vad_proj(vad_feats)  # [B,3,2H]
        K, V = torch.split(KV, self.hidden_dim, dim=-1)  # ä½¿ç”¨ self.hidden_dim
        
        attn_out, attn_weights = self.attn(Q, K, V, need_weights=True)
        return attn_out, attn_weights 

class GatedFusion(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        # é—¨æ§ç½‘ç»œï¼šç”Ÿæˆä¸€ä¸ªæ›´æ–°é—¨
        self.update_gate = nn.Sequential(
            nn.Linear(hidden_dim * 2, 64),  # è¾“å…¥æ˜¯æ‹¼æ¥åçš„ç‰¹å¾
            nn.ReLU(),
            nn.Linear(64, hidden_dim),      # è¾“å‡ºä¸ç‰¹å¾åŒç»´åº¦
            nn.Sigmoid()                    # è¾“å‡º0-1ä¹‹é—´çš„é—¨æ§å€¼
        )
        self.transform = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim)
        )
        
    def forward(self, attn_out, emo_feat):
        """
        å‚æ•°:
            attn_out: [B,1,H] -> å°†è¢« squeeze ä¸º [B,H]
            emo_feat: [B,H] ä¸»ä»»åŠ¡ç‰¹å¾
        """
        attn_out = attn_out.squeeze(1)  # [B, H]
        
        # 1. ç”Ÿæˆæ›´æ–°é—¨
        # å°†ä¸¤ä¸ªç‰¹å¾æ‹¼æ¥èµ·æ¥ä½œä¸ºé—¨æ§ç½‘ç»œçš„è¾“å…¥
        gate_input = torch.cat([emo_feat, attn_out], dim=-1)  # [B, 2*H]
        update_gate = self.update_gate(gate_input)  # [B, H]ï¼Œæ¯ä¸ªç‰¹å¾ç»´åº¦ä¸€ä¸ªé—¨æ§å€¼
        
        # 2. é—¨æ§èåˆï¼šç±»ä¼¼äº GRU çš„æ›´æ–°æœºåˆ¶
        # fused = (1 - update_gate) * emo_feat + update_gate * attn_out
        fused = emo_feat + update_gate * (attn_out - emo_feat)
        
        return self.transform(fused), update_gate  # è¿”å›èåˆç‰¹å¾å’Œé—¨æ§å€¼

def count_parameters(model):
    total_params = 0
    for name, parameter in model.named_parameters():
        param = parameter.numel()
        print(f"{name}: {param}")
        total_params += param
    print(f"\nTotal number of parameters: {total_params}")

@torch.no_grad()
def validate_and_test(model, data_loader, device, num_classes):
    model.eval()

    # === æƒ…æ„Ÿä»»åŠ¡æŒ‡æ ‡ï¼ˆä¿æŒä¸å˜ï¼‰===
    emotion_correct, emotion_total = 0, 0
    emotion_unweighted_correct = [0] * num_classes
    emotion_unweighted_total = [0] * num_classes
    tp = [0] * num_classes
    fp = [0] * num_classes
    fn = [0] * num_classes

    # === VAD: æ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®å€¼ï¼ˆç”¨äºå…¨å±€ CCCï¼‰===
    all_valence_true, all_valence_pred = [], []
    all_arousal_true, all_arousal_pred = [], []
    all_dominance_true, all_dominance_pred = [], []

    # æ³¨æ„åŠ›æƒé‡ï¼ˆå¯é€‰ï¼‰
    all_attn_weights = []
    all_fusion_weights = []

    for batch in data_loader:
        ids, net_input, labels = batch["id"], batch["net_input"], batch["labels"]
        feats = net_input["feats"]
        speech_padding_mask = net_input["padding_mask"]
        vad_targets = batch["vad_labels"].to(device)  # [B, 3]

        feats = feats.to(device)
        speech_padding_mask = speech_padding_mask.to(device)
        labels = labels.to(device)

        emotion_out, valence_out, arousal_out, dominance_out, attn_weights, fusion_weights = model(feats, speech_padding_mask)
         
        # æƒ…æ„Ÿé¢„æµ‹ï¼ˆä¿æŒä¸å˜ï¼‰
        _, emotion_pred = torch.max(emotion_out.data, 1)
        emotion_total += labels.size(0)
        emotion_correct += (emotion_pred == labels).sum().item()
        
        for i in range(len(labels)):
            true_label = labels[i].item()
            pred_label = emotion_pred[i].item()
            emotion_unweighted_total[true_label] += 1
            if pred_label == true_label:
                emotion_unweighted_correct[true_label] += 1
                tp[true_label] += 1
            else:
                fp[pred_label] += 1
                fn[true_label] += 1

        # === VAD: æ”¶é›†è¿ç»­é¢„æµ‹å€¼å’ŒçœŸå®å€¼ ===
        valence_pred = valence_out.squeeze(-1)  # [B]
        arousal_pred = arousal_out.squeeze(-1)
        dominance_pred = dominance_out.squeeze(-1)

        all_valence_true.append(vad_targets[:, 0])
        all_valence_pred.append(valence_pred)
        all_arousal_true.append(vad_targets[:, 1])
        all_arousal_pred.append(arousal_pred)
        all_dominance_true.append(vad_targets[:, 2])
        all_dominance_pred.append(dominance_pred)

        # æ”¶é›†æƒé‡ï¼ˆå¯é€‰ï¼‰
        all_attn_weights.append(attn_weights.squeeze(1).cpu().numpy())
        all_fusion_weights.append(fusion_weights.cpu().numpy())

    # === æƒ…æ„ŸæŒ‡æ ‡ï¼ˆä¸å˜ï¼‰===
    emotion_wa = emotion_correct / emotion_total * 100
    emotion_ua = compute_unweighted_accuracy(emotion_unweighted_correct, emotion_unweighted_total) * 100
    weighted_f1 = compute_weighted_f1(tp, fp, fn, emotion_unweighted_total) * 100

    # === VAD: è®¡ç®— CCC ===
    def compute_ccc(true_list, pred_list):
        if len(true_list) == 0:
            return 0.0
        true_all = torch.cat(true_list, dim=0)
        pred_all = torch.cat(pred_list, dim=0)
        return concordance_correlation_coefficient(true_all, pred_all).item()

    valence_ccc = compute_ccc(all_valence_true, all_valence_pred)
    arousal_ccc = compute_ccc(all_arousal_true, all_arousal_pred)
    dominance_ccc = compute_ccc(all_dominance_true, all_dominance_pred)
    vad_ccc = [valence_ccc, arousal_ccc, dominance_ccc]

    # ï¼ˆå¯é€‰ï¼‰æ‰“å°æ³¨æ„åŠ›åˆ†æ
    if all_attn_weights:
        all_attn_weights = np.concatenate(all_attn_weights, axis=0)
        avg_attn_weights = all_attn_weights.mean(axis=0)
        print(f"\nğŸ“Š Cross-Attention Weights: Valence={avg_attn_weights[0]:.4f}, "
              f"Arousal={avg_attn_weights[1]:.4f}, Dominance={avg_attn_weights[2]:.4f}")

    return emotion_wa, emotion_ua, weighted_f1, vad_ccc

def concordance_correlation_coefficient(y_true, y_pred):
    """
    Compute CCC for two 1D tensors.
    Returns a scalar CCC value.
    """
    if y_true.numel() < 2:
        return torch.tensor(0.0, device=y_true.device)
    
    mean_true = torch.mean(y_true)
    mean_pred = torch.mean(y_pred)
    
    var_true = torch.var(y_true, unbiased=False)
    var_pred = torch.var(y_pred, unbiased=False)
    
    cov = torch.mean((y_true - mean_true) * (y_pred - mean_pred))
    
    ccc = (2 * cov) / (var_true + var_pred + (mean_true - mean_pred) ** 2 + 1e-8)
    return ccc

def train_one_epoch(model, optimizer, criterion, train_loader, device, task_weights):
    model.train()
    total_loss = 0
    emotion_loss_total = 0
    # åˆ†åˆ«è®°å½•ä¸‰ä¸ªVADä»»åŠ¡çš„æŸå¤±
    valence_loss_total = 0
    arousal_loss_total = 0
    dominance_loss_total = 0
    
    for batch in train_loader:
        ids, net_input, labels = batch["id"], batch["net_input"], batch["labels"]
        feats = net_input["feats"]
        speech_padding_mask = net_input["padding_mask"]

        feats = feats.to(device)
        speech_padding_mask = speech_padding_mask.to(device)
        labels = labels.to(device)
        vad_labels = batch["vad_labels"].to(device)  # [B, 3]
        
        optimizer.zero_grad()
        
        # æ¨¡å‹å‰å‘ä¼ æ’­
        emotion_out, valence_out, arousal_out, dominance_out,_, _ = model(feats, speech_padding_mask)
        
        # è®¡ç®—æƒ…æ„Ÿä»»åŠ¡æŸå¤±
        emotion_loss = criterion(emotion_out, labels.long())
        
        valence_pred = valence_out.squeeze(-1)  # [B, 1] -> [B]
        valence_loss = F.mse_loss(valence_pred, vad_labels[:, 0])        

        arousal_pred = arousal_out.squeeze(-1)
        arousal_loss = F.mse_loss(arousal_pred, vad_labels[:, 1])

        dominance_pred = dominance_out.squeeze(-1)
        dominance_loss = F.mse_loss(dominance_pred, vad_labels[:, 2])

        # åŠ æƒæ€»æŸå¤±
        total_loss_batch = (
            task_weights[0] * emotion_loss +
            task_weights[1] * valence_loss +
            task_weights[2] * arousal_loss +
            task_weights[3] * dominance_loss
        )
        
        total_loss_batch.backward()
        optimizer.step()
        
        # ç´¯åŠ å„é¡¹æŸå¤±
        total_loss += total_loss_batch.item()
        emotion_loss_total += emotion_loss.item()
        valence_loss_total += valence_loss.item()
        arousal_loss_total += arousal_loss.item()
        dominance_loss_total += dominance_loss.item()
    
    # è®¡ç®—å¹³å‡æŸå¤±
    num_batches = len(train_loader)
    return (
        total_loss / num_batches, 
        emotion_loss_total / num_batches,
        [  # è¿”å›VADä¸‰ä¸ªç»´åº¦çš„å•ç‹¬æŸå¤±
            valence_loss_total / num_batches,
            arousal_loss_total / num_batches,
            dominance_loss_total / num_batches
        ]
    )

@hydra.main(config_path='config', config_name='default.yaml')
def train_iemocap(cfg: DictConfig):
    # æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„
    emotion_dict = {'ang': 0, 'hap': 1, 'neu': 2, 'sad': 3}

    
    n_samples = [1085, 1023, 1151, 1031, 1241]  # Session1, 2, 3, 4, 5
    idx_sessions = [0, 1, 2, 3, 4]

    test_wa_avg, test_ua_avg, test_vad_ccc_avg = 0., 0., [0., 0., 0.]
    test_f1_avg = 0.0

    for fold in idx_sessions:  # extract the $fold$th as test set
        # torch.manual_seed(cfg.common.seed)    
        #éšæœºç§å­è®¾ç«‹
        seed = cfg.common.seed
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        
        logger.info(f"------Now it's {fold+1}th fold------")
        
        val_wa_history = [] 
        prev_emo_loss = 1.0  # åˆå§‹ä¸»ä»»åŠ¡æŸå¤±
        prev_vad_losses = [1.0, 1.0, 1.0]  # åˆå§‹VADæŸå¤±  
        
        device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")
        torch.cuda.empty_cache()
        
        # åŠ è½½æ•°æ®é›†
        dataset = load_ssl_features(cfg.dataset.feat_path, emotion_dict)

        test_len = n_samples[fold] 
        test_idx_start = sum(n_samples[:fold])
        test_idx_end = test_idx_start + test_len 
        train_loader, val_loader, test_loader = train_valid_test_iemocap_dataloader(
            dataset,
            cfg.dataset.batch_size,
            test_idx_start,
            test_idx_end,
            eval_is_test=cfg.dataset.eval_is_test,
        )

        model = BaseModel(input_dim=1024, num_classes=len(emotion_dict))
        model = model.to(device)

        
        optimizer = optim.Adam(
            model.parameters(), 
            lr=cfg.optimization.lr, 
            weight_decay=cfg.optimization.get('weight_decay', 1e-4)  # æ·»åŠ æƒé‡è¡°å‡
        )
        scheduler = optim.lr_scheduler.CyclicLR(
            optimizer, 
            base_lr=cfg.optimization.lr, 
            max_lr=3e-3, 
            step_size_up=20,
            cycle_momentum=False 
        )
        criterion = nn.CrossEntropyLoss()

        # å®šä¹‰è®­ç»ƒé˜¶æ®µå‚æ•°
        phase1_epochs = 10  # é˜¶æ®µ1çš„epochæ•°
        phase2_total_epochs = max(0, cfg.optimization.epoch - phase1_epochs)
        
        # è®¡ç®—é˜¶æ®µ2çš„è§£å†»æ­¥éª¤
        if phase2_total_epochs > 0:
            step_interval = phase2_total_epochs // 3
            step1_epoch = phase1_epochs  # å¼€å§‹è§£å†»vad_adapters
            step2_epoch = phase1_epochs + step_interval  # å¼€å§‹è§£å†»cross_attention
            step3_epoch = phase1_epochs + 2 * step_interval  # å¼€å§‹è§£å†»fusion
        else:
            step1_epoch = step2_epoch = step3_epoch = phase1_epochs

        best_val_wa = 0
        best_val_wa_epoch = 0
        save_dir = os.path.join(str(Path.cwd()), f"model_{fold+1}.pth")
        
        for epoch in range(cfg.optimization.epoch):
            
            # åŠ¨æ€è°ƒæ•´å‚æ•°å†»ç»“çŠ¶æ€å’Œä»»åŠ¡æƒé‡
            if epoch < phase1_epochs:
                # é˜¶æ®µ1: ä»…è®­ç»ƒä¸»ä»»åŠ¡ï¼Œå†»ç»“æ‰€æœ‰VADç›¸å…³å‚æ•°
                # å†»ç»“VADé€‚é…å™¨
                for param in model.vad_adapters.parameters():
                    param.requires_grad = False
                # å†»ç»“VADè¾“å‡ºå±‚
                for param in model.valence_head.parameters():
                    param.requires_grad = False
                for param in model.arousal_head.parameters():
                    param.requires_grad = False
                for param in model.dominance_head.parameters():
                    param.requires_grad = False
                # å†»ç»“äº¤å‰æ³¨æ„åŠ›
                for param in model.cross_attention.parameters():
                    param.requires_grad = False
                # å†»ç»“é—¨æ§èåˆæ¨¡å—
                for param in model.fusion.parameters():
                    param.requires_grad = False
                
                # ä»…ä½¿ç”¨æƒ…æ„Ÿä»»åŠ¡æŸå¤±
                task_weights = [1.0, 0.0, 0.0, 0.0]
                logger.info(f"Epoch {epoch+1}: PHASE 1 - Only training emotion task, freezing all VAD components")
            else:   
               # é˜¶æ®µ2: æ¸è¿›å¼è§£å†»å‚æ•°
                if epoch < step2_epoch:
                    # æ­¥éª¤1: è§£å†»vad_adaptersï¼ˆä»»åŠ¡é€‚é…å±‚ï¼‰
                    for param in model.vad_adapters.parameters():
                        param.requires_grad = True
                    for param in model.valence_head.parameters():
                        param.requires_grad = True
                    for param in model.arousal_head.parameters():
                        param.requires_grad = True
                    for param in model.dominance_head.parameters():
                        param.requires_grad = True
                    # ä¿æŒcross_attentionå’Œfusionå†»ç»“
                    for param in model.cross_attention.parameters():
                        param.requires_grad = False
                    for param in model.fusion.parameters():
                        param.requires_grad = False
                    logger.info(f"Epoch {epoch+1}: PHASE 2 - Step 1: Unfrozen VAD adapters")
                elif epoch < step3_epoch:
                    # æ­¥éª¤2: è§£å†»cross_attentionï¼ˆæ³¨æ„åŠ›æ¨¡å—ï¼‰
                    for param in model.cross_attention.parameters():
                        param.requires_grad = True
                    # ä¿æŒfusionå†»ç»“
                    for param in model.fusion.parameters():
                        param.requires_grad = False
                    logger.info(f"Epoch {epoch+1}: PHASE 2 - Step 2: Unfrozen cross-attention")
                else:
                    # æ­¥éª¤3: è§£å†»fusionï¼ˆé—¨æ§æ¨¡å—ï¼‰
                    for param in model.fusion.parameters():
                        param.requires_grad = True
                    logger.info(f"Epoch {epoch+1}: PHASE 2 - Step 3: Unfrozen fusion module")
                
     
                phase2_epoch = epoch - phase1_epochs
                if phase2_total_epochs > 0:
                    ratio = min(phase2_epoch / phase2_total_epochs, 1.0)
                else:
                    ratio = 1.0
                
                w_emotion = 1 - 0.4 * ratio  
                total_vad_weight = 0.0 + 0.4 * ratio  
                w_v = w_a = w_d = total_vad_weight / 3.0  
                
                task_weights = [w_emotion, w_v, w_a, w_d]
                logger.info(f"Epoch {epoch+1}: PHASE 2 - Task Weights: Emo={w_emotion:.2f}, VAD Total={total_vad_weight:.2f} (V={w_v:.2f}, A={w_a:.2f}, D={w_d:.2f})")
            
            # è®­ç»ƒæ—¶ä¼ å…¥VADæƒé‡
            total_loss, emotion_loss, vad_loss = train_one_epoch(
                model, optimizer, criterion, train_loader, device, 
                task_weights
            )
            scheduler.step()
            v_loss, a_loss, d_loss = vad_loss 
            
            # æ›´æ–°æŸå¤±è®°å½•
            prev_emo_loss = emotion_loss
            prev_vad_losses = [v_loss, a_loss, d_loss] 

            # éªŒè¯
            val_wa, val_ua, val_f1, val_vad_ccc = validate_and_test(
                model, val_loader, device, num_classes=len(emotion_dict)
            )
            val_wa_history.append(val_wa)

            # æ—¥å¿—è¾“å‡ºæƒé‡ä¿¡æ¯
            logger.info(f"Epoch {epoch+1} Task Weights: "
                        f"Emo={task_weights[0]:.2f}, "
                        f"Val={task_weights[1]:.2f}, "
                        f"Aro={task_weights[2]:.2f}, "
                        f"Dom={task_weights[3]:.2f}")

            
            if val_wa > best_val_wa:
                best_val_wa = val_wa
                best_val_wa_epoch = epoch
                torch.save(model.state_dict(), save_dir)

            # ä¿®æ”¹åçš„æ—¥å¿—è¯­å¥
            logger.info(f"Epoch {epoch+1}: "
                        f"Total Loss: {total_loss:.4f}, "
                        f"Emotion Loss: {emotion_loss:.4f}, "
                        f"VAD Loss: V={vad_loss[0]:.4f}, A={vad_loss[1]:.4f}, D={vad_loss[2]:.4f}, "  # åˆ†åˆ«è®¿é—®åˆ—è¡¨å…ƒç´ 
                        f"Val WA: {val_wa:.3f}%, "
                        f"Val UA: {val_ua:.3f}%, "
                        f"Val F1: {val_f1:.3f}%, "  # âœ… æ–°å¢ F1
                        f"Val VAD CCC: V={val_vad_ccc[0]:.4f}, A={val_vad_ccc[1]:.4f}, D={val_vad_ccc[2]:.4f}")

        # æµ‹è¯•
        ckpt = torch.load(save_dir)
        model.load_state_dict(ckpt, strict=True)
        test_wa, test_ua,test_f1, test_vad_ccc = validate_and_test(
            model, test_loader, device, num_classes=len(emotion_dict)
        )
        
        logger.info(f"The {fold+1}th Fold at epoch {best_val_wa_epoch + 1}: "
                    f"Test WA {test_wa:.3f}%, "
                    f"Test UA {test_ua:.3f}%, "
                    f"Test F1 {test_f1:.3f}%, "  # âœ… æ–°å¢ F1
                    f"Test VAD CCC: V={test_vad_ccc[0]:.4f}, A={test_vad_ccc[1]:.4f}, D={test_vad_ccc[2]:.4f}")
        
        test_wa_avg += test_wa
        test_ua_avg += test_ua
        test_f1_avg += test_f1
        test_vad_ccc_avg = [test_vad_ccc_avg[i] + test_vad_ccc[i] for i in range(3)]

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    num_folds = len(idx_sessions)
    test_wa_avg /= num_folds
    test_ua_avg /= num_folds
    test_vad_ccc_avg = [ccc / num_folds for ccc in test_vad_ccc_avg]    


    logger.info(f"Average Results: "
                f"WA: {test_wa_avg:.3f}%, "
                f"UA: {test_ua_avg:.3f}%, "
                f"F1: {test_f1_avg / len(idx_sessions):.3f}%, " 
                f"VAD CCC: V={test_vad_ccc_avg[0]:.4f}, A={test_vad_ccc_avg[1]:.4f}, D={test_vad_ccc_avg[2]:.4f}")

if __name__ == '__main__':
    train_iemocap()

